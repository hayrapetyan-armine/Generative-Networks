{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a6e8903",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ced51a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "import os \n",
    "from tqdm import tqdm\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89d2dfd",
   "metadata": {},
   "source": [
    "## Scale Convolution By Equalized Learning Rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd9ca610",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EqualizedLR_Conv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch module implementing a convolutional layer with equalized learning rate.\n",
    "\n",
    "    Args:\n",
    "        in_ch (int): Number of input channels.\n",
    "        out_ch (int): Number of output channels.\n",
    "        kernel_size (Tuple[int, int]): Size of the convolutional kernel (height, width).\n",
    "        stride (int, optional): Stride of the convolution operation. Defaults to 1.\n",
    "        padding (int, optional): Amount of padding applied to the input. Defaults to 0.\n",
    "\n",
    "    Attributes:\n",
    "        padding (int): Amount of padding applied to the input.\n",
    "        stride (int): Stride of the convolution operation.\n",
    "        scale (float): Scaling factor used for equalized learning rate.\n",
    "        weight (Tensor): Trainable parameter representing the convolutional weights.\n",
    "        bias (Tensor): Trainable parameter representing the convolutional biases.\n",
    "\n",
    "    Methods:\n",
    "        forward(x): Performs the forward pass of the convolutional layer.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, kernel_size, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.scale = np.sqrt(2/(in_ch * kernel_size[0] * kernel_size[1]))\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(out_ch, in_ch, *kernel_size))\n",
    "        self.bias = Parameter(torch.Tensor(out_ch))\n",
    "\n",
    "        nn.init.normal_(self.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.conv2d(x, self.weight*self.scale, self.bias, self.stride, self.padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18e9655",
   "metadata": {},
   "source": [
    "## Create Pixel Normalization Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5bd0257",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pixel_norm(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch module implementing pixel-wise feature normalization.\n",
    "\n",
    "    This module performs pixel-wise feature normalization on the input tensor,\n",
    "    where each pixel is normalized by its L2 norm across the channel dimension.\n",
    "\n",
    "    Methods:\n",
    "        forward(a: Tensor) -> Tensor:\n",
    "            Performs pixel-wise feature normalization on the input tensor.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, a):\n",
    "        b = a / torch.sqrt(torch.sum(a**2, dim=1, keepdim=True) + 1e-8)\n",
    "        return b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72d309d",
   "metadata": {},
   "source": [
    "## Create Minibatch STD Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a33564",
   "metadata": {},
   "source": [
    "The **Minibatch_std** module is used as a building block in deep neural networks to help prevent mode collapse in generative models. By computing and appending the standard deviation of feature maps to each sample in a mini-batch, the module can encourage the generator to produce more diverse outputs by penalizing mode collapse and encouraging the generator to explore a wider range of possible feature map values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "abe9f4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Minibatch_std(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch module that computes the standard deviation of feature maps across samples in a mini-batch and appends\n",
    "    the resulting value as an additional feature map to each sample.\n",
    "\n",
    "    This module can be used as a building block in deep neural networks to help prevent mode collapse in generative\n",
    "    models. By computing and appending the standard deviation of feature maps to each sample in a mini-batch, the module\n",
    "    can encourage the generator to produce more diverse outputs by penalizing mode collapse and encouraging the\n",
    "    generator to explore a wider range of possible feature map values.\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Shape:\n",
    "        - Input: `(batch_size, num_channels, height, width)`\n",
    "        - Output: `(batch_size, num_channels+1, height, width)`\n",
    "\n",
    "    Example:\n",
    "        >> x = torch.randn(32, 128, 4, 4)\n",
    "        >> minibatch_std = Minibatch_std()\n",
    "        >> y = minibatch_std(x)\n",
    "        >> print(y.shape)\n",
    "        torch.Size([32, 129, 4, 4])\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = list(x.size())\n",
    "        size[1] = 1\n",
    "\n",
    "        std = torch.std(x, dim=0)\n",
    "        mean = torch.mean(std)\n",
    "        return torch.cat((x, mean.repeat(size)),dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436d6a9d",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3bc7d5",
   "metadata": {},
   "source": [
    "### FromRGB Operation <br>\n",
    "The \"FromRGB\" operation is applied to the input images at the lower-resolution stages of the generator network.<br>\n",
    "\n",
    "The operation consists of a convolutional layer with a kernel size of 1x1, followed by a leaky ReLU activation function. The purpose of this operation is to convert the input images from their original RGB color space to a feature map representation that can be processed by the generator network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63118d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FromRGB(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = EqualizedLR_Conv2d(in_ch, out_ch, kernel_size=(1, 1), stride=(1, 1))\n",
    "        self.relu = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return self.relu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37dea8e",
   "metadata": {},
   "source": [
    "### ToRGB Operation<br>\n",
    "The \"ToRGB\" operation is applied to the output of the generator network at each resolution stage to produce the final output image.<br>\n",
    "\n",
    "The operation consists of a convolutional layer with a kernel size of 1x1, without any activation function. The purpose of this operation is to convert the feature map representation of the generator's output to an RGB color space image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc8fc061",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToRGB(nn.Module):\n",
    "    \"\"\"PyTorch module to perform the \"ToRGB\" operation in the ProGAN architecture.\n",
    "\n",
    "    The operation converts the feature map representation of the generator's output to an RGB color space image.\n",
    "\n",
    "    Args:\n",
    "        in_ch (int): The number of input channels.\n",
    "        out_ch (int): The number of output channels.\n",
    "\n",
    "    Attributes:\n",
    "        conv (EqualizedLR_Conv2d): The equalized learning rate convolutional layer.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = EqualizedLR_Conv2d(in_ch, out_ch, kernel_size=(1,1), stride=(1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Applies the \"ToRGB\" operation to the input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after applying the \"ToRGB\" operation.\n",
    "        \n",
    "        \"\"\"\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82437f7",
   "metadata": {},
   "source": [
    "### Generator Block\n",
    "The purpose of this block is to gradually upsample the feature maps and increase the number of channels to produce a higher resolution image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "567f0565",
   "metadata": {},
   "outputs": [],
   "source": [
    "class G_Block(nn.Module):\n",
    "    \"\"\"\n",
    "    A class for a generator block.\n",
    "\n",
    "    Args:\n",
    "    - in_ch (int): number of channels in the input tensor.\n",
    "    - out_ch (int): number of channels in the output tensor.\n",
    "    - initial_block (bool): whether this block is the initial block of the generator or not.\n",
    "\n",
    "    Attributes:\n",
    "    - upsample (nn.Module): a module for upsampling the input tensor by a factor of 2 using nearest neighbor interpolation.\n",
    "      If this is the initial block, this attribute is set to None.\n",
    "    - conv1 (EqualizedLR_Conv2d): a convolutional layer that applies a learned convolution operation on the input tensor.\n",
    "      The kernel size is (4, 4) if this is the initial block, otherwise it is (3, 3). The stride is (1, 1) and the padding is (3, 3) if this is the initial block,\n",
    "      otherwise it is (1, 1).\n",
    "    - conv2 (EqualizedLR_Conv2d): another convolutional layer that applies a learned convolution operation on the output tensor of conv1.\n",
    "      The kernel size is (3, 3) and the stride is (1, 1) and the padding is (1, 1).\n",
    "    - relu (nn.LeakyReLU): a module for applying the LeakyReLU activation function with a negative slope of 0.2.\n",
    "    - pixelwisenorm (Pixel_norm): a module that performs pixel-wise normalization on the output tensor of relu.\n",
    "\n",
    "    Methods:\n",
    "    - forward(x): applies the forward pass of the generator block on the input tensor x.\n",
    "\n",
    "    Returns:\n",
    "    - x (torch.Tensor): the output tensor of the generator block.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, initial_block=False):\n",
    "        super().__init__()\n",
    "        if initial_block:\n",
    "            self.upsample = None\n",
    "            self.conv1 = EqualizedLR_Conv2d(in_ch, out_ch, kernel_size=(4, 4), stride=(1, 1), padding=(3, 3))\n",
    "        else:\n",
    "            self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "            self.conv1 = EqualizedLR_Conv2d(in_ch, out_ch, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv2 = EqualizedLR_Conv2d(out_ch, out_ch, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.relu = nn.LeakyReLU(0.2)\n",
    "        self.pixelwisenorm = Pixel_norm()\n",
    "        nn.init.normal_(self.conv1.weight)\n",
    "        nn.init.normal_(self.conv2.weight)\n",
    "        nn.init.zeros_(self.conv1.bias)\n",
    "        nn.init.zeros_(self.conv2.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.upsample is not None:\n",
    "            x = self.upsample(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pixelwisenorm(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pixelwisenorm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39f0e52",
   "metadata": {},
   "source": [
    "### Discriminator Block\n",
    "Represents a block of layers for the discriminator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d1a24249",
   "metadata": {},
   "outputs": [],
   "source": [
    "class D_Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, initial_block=False):\n",
    "        \"\"\"\n",
    "        Discriminator block for the discriminator network.\n",
    "\n",
    "        Args:\n",
    "        - in_ch (int): Number of input channels to the block.\n",
    "        - out_ch (int): Number of output channels of the block.\n",
    "        - initial_block (bool): Whether this block is the initial block in the discriminator network.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if initial_block:\n",
    "            self.minibatchstd = Minibatch_std()\n",
    "            self.conv1 = EqualizedLR_Conv2d(in_ch+1, out_ch, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "            self.conv2 = EqualizedLR_Conv2d(out_ch, out_ch, kernel_size=(4, 4), stride=(1, 1))\n",
    "            self.outlayer = nn.Sequential(\n",
    "                                    nn.Flatten(),\n",
    "                                    nn.Linear(out_ch, 1)\n",
    "                                    )\n",
    "        else:\n",
    "            self.minibatchstd = None\n",
    "            self.conv1 = EqualizedLR_Conv2d(in_ch, out_ch, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "            self.conv2 = EqualizedLR_Conv2d(out_ch, out_ch, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "            self.outlayer = nn.AvgPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "        self.relu = nn.LeakyReLU(0.2)\n",
    "        nn.init.normal_(self.conv1.weight)\n",
    "        nn.init.normal_(self.conv2.weight)\n",
    "        nn.init.zeros_(self.conv1.bias)\n",
    "        nn.init.zeros_(self.conv2.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs forward pass on the discriminator block.\n",
    "\n",
    "        Args:\n",
    "        - x (tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "        - Output tensor.\n",
    "        \"\"\"\n",
    "        if self.minibatchstd is not None:\n",
    "            x = self.minibatchstd(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.outlayer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988c77e4",
   "metadata": {},
   "source": [
    "### Generator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "26783fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"A generator network for progressive growing GAN.\n",
    "\n",
    "    Args:\n",
    "        latent_size (int): The size of the latent space vector.\n",
    "        out_res (int): The output resolution of the generated image.\n",
    "\n",
    "    Attributes:\n",
    "        depth (int): The depth of the current network.\n",
    "        alpha (float): The fading coefficient for the current network.\n",
    "        fade_iters (float): The number of iterations to complete the fade-in phase.\n",
    "        upsample (nn.Module): An upsampling layer to increase the spatial resolution of the tensor.\n",
    "        current_net (nn.ModuleList): A list of G_Block layers that make up the current network.\n",
    "        toRGBs (nn.ModuleList): A list of ToRGB layers that convert the output tensor to an RGB image.\n",
    "\n",
    "    Methods:\n",
    "        forward(x): Forward pass through the generator network.\n",
    "        growing_net(num_iters): Add a new block to the generator network and start the fade-in phase.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_size, out_res):\n",
    "        super().__init__()\n",
    "        self.depth = 1\n",
    "        self.alpha = 1\n",
    "        self.fade_iters = 0\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.current_net = nn.ModuleList([G_Block(latent_size, latent_size, initial_block=True)])\n",
    "        self.toRGBs = nn.ModuleList([ToRGB(latent_size, 3)])\n",
    "        # __add_layers(out_res)\n",
    "        for d in range(2, int(np.log2(out_res))):\n",
    "            if d < 6:\n",
    "                ## low res blocks 8x8, 16x16, 32x32 with 512 channels\n",
    "                in_ch, out_ch = 512, 512\n",
    "            else:\n",
    "                ## from 64x64(5th block), the number of channels halved for each block\n",
    "                in_ch, out_ch = int(512 / 2**(d - 6)), int(512 / 2**(d - 5))\n",
    "            self.current_net.append(G_Block(in_ch, out_ch))\n",
    "            self.toRGBs.append(ToRGB(out_ch, 3))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.current_net[:self.depth-1]:\n",
    "            x = block(x)\n",
    "        out = self.current_net[self.depth-1](x)\n",
    "        x_rgb = self.toRGBs[self.depth-1](out)\n",
    "        if self.alpha < 1:\n",
    "            x_old = self.upsample(x)\n",
    "            old_rgb = self.toRGBs[self.depth-2](x_old)\n",
    "            x_rgb = (1-self.alpha)* old_rgb + self.alpha * x_rgb\n",
    "\n",
    "            self.alpha += self.fade_iters\n",
    "\n",
    "        return x_rgb\n",
    "\n",
    "\n",
    "    def growing_net(self, num_iters):\n",
    "\n",
    "        self.fade_iters = 1/num_iters\n",
    "        self.alpha = 1/num_iters\n",
    "\n",
    "        self.depth += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02120c3",
   "metadata": {},
   "source": [
    "### Discriminator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67eb91fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    A class representing a discriminator network for the progressive GAN.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    latent_size : int\n",
    "        The size of the noise vector input to the generator.\n",
    "    out_res : int\n",
    "        The output resolution of the generator.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    depth : int\n",
    "        The current depth of the network.\n",
    "    alpha : float\n",
    "        The current alpha value for fade-in training.\n",
    "    fade_iters : float\n",
    "        The number of iterations for fade-in training.\n",
    "    downsample : nn.AvgPool2d\n",
    "        A module that performs average pooling.\n",
    "    current_net : nn.ModuleList\n",
    "        A list of D_Block modules representing the current depth of the network.\n",
    "    fromRGBs : nn.ModuleList\n",
    "        A list of FromRGB modules representing the current depth of the input.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_size, out_res):\n",
    "        super().__init__()\n",
    "        self.depth = 1\n",
    "        self.alpha = 1\n",
    "        self.fade_iters = 0\n",
    "\n",
    "        self.downsample = nn.AvgPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "        self.current_net = nn.ModuleList([D_Block(latent_size, latent_size, initial_block=True)])\n",
    "        self.fromRGBs = nn.ModuleList([FromRGB(3, latent_size)])\n",
    "        for d in range(2, int(np.log2(out_res))):\n",
    "            if d < 6:\n",
    "                in_ch, out_ch = 512, 512\n",
    "            else:\n",
    "                in_ch, out_ch = int(512 / 2**(d - 5)), int(512 / 2**(d - 6))\n",
    "            self.current_net.append(D_Block(in_ch, out_ch))\n",
    "            self.fromRGBs.append(FromRGB(3, in_ch))\n",
    "\n",
    "    def forward(self, x_rgb):\n",
    "        x = self.fromRGBs[self.depth-1](x_rgb)\n",
    "\n",
    "        x = self.current_net[self.depth-1](x)\n",
    "        if self.alpha < 1:\n",
    "\n",
    "            x_rgb = self.downsample(x_rgb)\n",
    "            x_old = self.fromRGBs[self.depth-2](x_rgb)\n",
    "            x = (1-self.alpha)* x_old + self.alpha * x\n",
    "            self.alpha += self.fade_iters\n",
    "        for block in reversed(self.current_net[:self.depth-1]):\n",
    "            x = block(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def growing_net(self, num_iters):\n",
    "\n",
    "        self.fade_iters = 1/num_iters\n",
    "        self.alpha = 1/num_iters\n",
    "\n",
    "        self.depth += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ac9597",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "caa7e783",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = {\n",
    "    'root': '/home/armine/Downloads/celebA/',\n",
    "    'epochs': 40,\n",
    "    'out_res': 128,\n",
    "    'resume': 0,\n",
    "    'cuda': True\n",
    "}\n",
    "\n",
    "root = opt['root']\n",
    "data_dir = root + 'celeba/'\n",
    "check_point_dir = root + 'check_points/'\n",
    "output_dir = root + 'output/'\n",
    "weight_dir = root + 'weight/'\n",
    "if not os.path.exists(check_point_dir):\n",
    "    os.makedirs(check_point_dir)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "if not os.path.exists(weight_dir):\n",
    "    os.makedirs(weight_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "db8fc18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The schedule contains [num of epoches for starting each size][batch size for each size][num of epoches]\n",
    "schedule = [[5, 15, 25 ,35, 40],[16, 16, 16, 8, 4],[5, 5, 5, 1, 1]]\n",
    "batch_size = schedule[1][0]\n",
    "growing = schedule[2][0]\n",
    "epochs = opt['epochs']\n",
    "latent_size = 512\n",
    "out_res = opt['out_res']\n",
    "lr = 1e-4\n",
    "lambd = 10\n",
    "\n",
    "device = torch.device('cuda:0' if (torch.cuda.is_available() and opt['cuda'])  else 'cpu')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize(out_res),\n",
    "        transforms.CenterCrop(out_res),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a588d11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_net = Discriminator(latent_size, out_res).to(device)\n",
    "G_net = Generator(latent_size, out_res).to(device)\n",
    "\n",
    "fixed_noise = torch.randn(16, latent_size, 1, 1, device=device)\n",
    "D_optimizer = optim.Adam(D_net.parameters(), lr=lr, betas=(0, 0.99))\n",
    "G_optimizer = optim.Adam(G_net.parameters(), lr=lr, betas=(0, 0.99))\n",
    "\n",
    "D_running_loss = 0.0\n",
    "G_running_loss = 0.0\n",
    "iter_num = 0\n",
    "\n",
    "D_epoch_losses = []\n",
    "G_epoch_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "305d0a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    print('Using ', torch.cuda.device_count(), 'GPUs')\n",
    "    D_net = nn.DataParallel(D_net)\n",
    "    G_net = nn.DataParallel(G_net)\n",
    "\n",
    "if opt['resume'] != 0:\n",
    "    check_point = torch.load(check_point_dir+'check_point_epoch_%i.pth' % opt['resume'])\n",
    "    fixed_noise = check_point['fixed_noise']\n",
    "    G_net.load_state_dict(check_point['G_net'])\n",
    "    D_net.load_state_dict(check_point['D_net'])\n",
    "    G_optimizer.load_state_dict(check_point['G_optimizer'])\n",
    "    D_optimizer.load_state_dict(check_point['D_optimizer'])\n",
    "    G_epoch_losses = check_point['G_epoch_losses']\n",
    "    D_epoch_losses = check_point['D_epoch_losses']\n",
    "    G_net.depth = check_point['depth']\n",
    "    D_net.depth = check_point['depth']\n",
    "    G_net.alpha = check_point['alpha']\n",
    "    D_net.alpha = check_point['alpha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "01f54890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Resolution: 4 x 4\n"
     ]
    }
   ],
   "source": [
    "c = next(x[0] for x in enumerate(schedule[0]) if x[1]>opt['resume'])-1\n",
    "batch_size = schedule[1][c]\n",
    "growing = schedule[2][c]\n",
    "dataset = datasets.ImageFolder(data_dir, transform=transform)\n",
    "# dataset = datasets.CelebA(data_dir, split='all', transform=transform)\n",
    "data_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "tot_iter_num = (len(dataset)/batch_size)\n",
    "G_net.fade_iters = (1-G_net.alpha)/(schedule[0][c+1]-opt['resume'])/(2*tot_iter_num)\n",
    "D_net.fade_iters = (1-D_net.alpha)/(schedule[0][c+1]-opt['resume'])/(2*tot_iter_num)\n",
    "\n",
    "size = 2**(G_net.depth+1)\n",
    "print(\"Output Resolution: %d x %d\" % (size, size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbcab86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(1+opt['resume'], opt['epochs']+1):\n",
    "    G_net.train()\n",
    "    D_epoch_loss = 0.0\n",
    "    G_epoch_loss = 0.0\n",
    "    if epoch-1 in schedule[0]:\n",
    "\n",
    "        if (2 **(G_net.depth +1) < out_res):\n",
    "            c = schedule[0].index(epoch-1)\n",
    "            batch_size = schedule[1][c]\n",
    "            growing = schedule[2][0]\n",
    "            data_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "            tot_iter_num = tot_iter_num = (len(dataset)/batch_size)\n",
    "            G_net.growing_net(growing*tot_iter_num)\n",
    "            D_net.growing_net(growing*tot_iter_num)\n",
    "            size = 2**(G_net.depth+1)\n",
    "            print(\"Output Resolution: %d x %d\" % (size, size))\n",
    "\n",
    "    print(\"epoch: %i/%i\" % (int(epoch), int(epochs)))\n",
    "    databar = tqdm(data_loader)\n",
    "\n",
    "    for i, samples in enumerate(databar):\n",
    "        ##  update D\n",
    "        if size != out_res:\n",
    "            samples = F.interpolate(samples[0], size=size).to(device)\n",
    "        else:\n",
    "            samples = samples[0].to(device)\n",
    "        D_net.zero_grad()\n",
    "        noise = torch.randn(samples.size(0), latent_size, 1, 1, device=device)\n",
    "        fake = G_net(noise)\n",
    "        fake_out = D_net(fake.detach())\n",
    "        real_out = D_net(samples)\n",
    "\n",
    "        ## Gradient Penalty\n",
    "\n",
    "        eps = torch.rand(samples.size(0), 1, 1, 1, device=device)\n",
    "        eps = eps.expand_as(samples)\n",
    "        x_hat = eps * samples + (1 - eps) * fake.detach()\n",
    "        x_hat.requires_grad = True\n",
    "        px_hat = D_net(x_hat)\n",
    "        grad = torch.autograd.grad(\n",
    "                                    outputs = px_hat.sum(),\n",
    "                                    inputs = x_hat, \n",
    "                                    create_graph=True\n",
    "                                    )[0]\n",
    "        grad_norm = grad.view(samples.size(0), -1).norm(2, dim=1)\n",
    "        gradient_penalty = lambd * ((grad_norm  - 1)**2).mean()\n",
    "\n",
    "        ###########\n",
    "\n",
    "        D_loss = fake_out.mean() - real_out.mean() + gradient_penalty\n",
    "\n",
    "        D_loss.backward()\n",
    "        D_optimizer.step()\n",
    "\n",
    "        ##\tupdate G\n",
    "\n",
    "        G_net.zero_grad()\n",
    "        fake_out = D_net(fake)\n",
    "\n",
    "        G_loss = - fake_out.mean()\n",
    "\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "\n",
    "        ##############\n",
    "\n",
    "        D_running_loss += D_loss.item()\n",
    "        G_running_loss += G_loss.item()\n",
    "\n",
    "        iter_num += 1\n",
    "\n",
    "\n",
    "        if i % 500== 0:\n",
    "            D_running_loss /= iter_num\n",
    "            G_running_loss /= iter_num\n",
    "            print('iteration : %d, gp: %.2f' % (i, gradient_penalty))\n",
    "            databar.set_description('D_loss: %.3f   G_loss: %.3f' % (D_running_loss ,G_running_loss))\n",
    "            iter_num = 0\n",
    "            D_running_loss = 0.0\n",
    "            G_running_loss = 0.0\n",
    "\n",
    "\n",
    "    D_epoch_losses.append(D_epoch_loss/tot_iter_num)\n",
    "    G_epoch_losses.append(G_epoch_loss/tot_iter_num)\n",
    "\n",
    "\n",
    "    check_point = {'G_net' : G_net.state_dict(), \n",
    "                   'G_optimizer' : G_optimizer.state_dict(),\n",
    "                   'D_net' : D_net.state_dict(),\n",
    "                   'D_optimizer' : D_optimizer.state_dict(),\n",
    "                   'D_epoch_losses' : D_epoch_losses,\n",
    "                   'G_epoch_losses' : G_epoch_losses,\n",
    "                   'fixed_noise': fixed_noise,\n",
    "                   'depth': G_net.depth,\n",
    "                   'alpha':G_net.alpha\n",
    "                   }\n",
    "    with torch.no_grad():\n",
    "        G_net.eval()\n",
    "        torch.save(check_point, check_point_dir + 'check_point_epoch_%d.pth' % (epoch))\n",
    "        torch.save(G_net.state_dict(), weight_dir + 'G_weight_epoch_%d.pth' %(epoch))\n",
    "        out_imgs = G_net(fixed_noise)\n",
    "        out_grid = make_grid(out_imgs, normalize=True, nrow=4, scale_each=True, padding=int(0.5*(2**G_net.depth))).permute(1,2,0)\n",
    "        plt.imshow(out_grid.cpu())\n",
    "        plt.savefig(output_dir + 'size_%i_epoch_%d' %(size ,epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6379741c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
